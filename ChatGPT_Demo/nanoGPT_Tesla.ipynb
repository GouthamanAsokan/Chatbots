{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTUUhSDGzHTa",
        "outputId": "d4b37d4c-2408-40cf-bd9b-48a382340e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 500, done.\u001b[K\n",
            "remote: Total 500 (delta 0), reused 0 (delta 0), pack-reused 500\u001b[K\n",
            "Receiving objects: 100% (500/500), 743.44 KiB | 18.13 MiB/s, done.\n",
            "Resolving deltas: 100% (295/295), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf78L8qQzj49",
        "outputId": "86c89e05-8874-4294-95ac-05bb50db40e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2022.6.2)\n",
            "Collecting requests>=2.26.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blobfile>=2\n",
            "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (1.26.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (3.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
            "Installing collected packages: tokenizers, requests, pycryptodomex, huggingface-hub, blobfile, transformers, tiktoken\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed blobfile-2.0.1 huggingface-hub-0.12.1 pycryptodomex-3.17 requests-2.28.2 tiktoken-0.3.0 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on tesla data\n",
        "!cd ./nanoGPT/data/tesla/ && python prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPtYcpW7zqqA",
        "outputId": "6d49973f-c790-4914-c582-fe95fa66e701"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 15,755 tokens\n",
            "val has 1,756 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train nanogpt on GPU, model in ./out. (300 iters seems to have lowest val loss) \n",
        "!cd ./nanoGPT/ && python train.py --dataset=tesla --n_layer=4 --n_head=4 --n_embd=64 --compile=False --block_size=64 --batch_size=8 --dtype=float16 --eval_interval=100 --eval_iters=100 --max_iters=300 --bias=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l9XtXoE0i5h",
        "outputId": "ec61b512-8990-4cde-906e-b100336f8b7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dataset = tesla\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 64\n",
            "Overriding: compile = False\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 8\n",
            "Overriding: dtype = float16\n",
            "Overriding: eval_interval = 100\n",
            "Overriding: eval_iters = 100\n",
            "Overriding: max_iters = 300\n",
            "Overriding: bias = True\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "number of parameters: 3.42M\n",
            "using fused AdamW: False\n",
            "step 0: train loss 10.8406, val loss 10.8440\n",
            "iter 0: loss 10.8486, time 5160.69ms, mfu -100.00%\n",
            "iter 1: loss 10.8509, time 555.00ms, mfu -100.00%\n",
            "iter 2: loss 10.8323, time 557.76ms, mfu -100.00%\n",
            "iter 3: loss 10.8454, time 558.49ms, mfu -100.00%\n",
            "iter 4: loss 10.8535, time 559.00ms, mfu -100.00%\n",
            "iter 5: loss 10.8417, time 559.11ms, mfu 0.24%\n",
            "iter 6: loss 10.8353, time 558.12ms, mfu 0.24%\n",
            "iter 7: loss 10.8343, time 562.33ms, mfu 0.24%\n",
            "iter 8: loss 10.8450, time 554.06ms, mfu 0.24%\n",
            "iter 9: loss 10.8252, time 562.09ms, mfu 0.24%\n",
            "iter 10: loss 10.8288, time 545.04ms, mfu 0.24%\n",
            "iter 11: loss 10.8459, time 556.62ms, mfu 0.24%\n",
            "iter 12: loss 10.8372, time 574.96ms, mfu 0.24%\n",
            "iter 13: loss 10.8475, time 560.98ms, mfu 0.24%\n",
            "iter 14: loss 10.8379, time 570.55ms, mfu 0.24%\n",
            "iter 15: loss 10.8282, time 555.75ms, mfu 0.24%\n",
            "iter 16: loss 10.8357, time 559.03ms, mfu 0.24%\n",
            "iter 17: loss 10.8369, time 546.45ms, mfu 0.24%\n",
            "iter 18: loss 10.8253, time 682.79ms, mfu 0.24%\n",
            "iter 19: loss 10.8350, time 739.96ms, mfu 0.23%\n",
            "iter 20: loss 10.8363, time 749.45ms, mfu 0.23%\n",
            "iter 21: loss 10.8404, time 754.81ms, mfu 0.22%\n",
            "iter 22: loss 10.8269, time 812.39ms, mfu 0.22%\n",
            "iter 23: loss 10.8310, time 839.59ms, mfu 0.21%\n",
            "iter 24: loss 10.8445, time 641.77ms, mfu 0.21%\n",
            "iter 25: loss 10.8173, time 560.82ms, mfu 0.22%\n",
            "iter 26: loss 10.8266, time 554.61ms, mfu 0.22%\n",
            "iter 27: loss 10.8197, time 555.50ms, mfu 0.22%\n",
            "iter 28: loss 10.8211, time 559.99ms, mfu 0.22%\n",
            "iter 29: loss 10.8270, time 568.00ms, mfu 0.22%\n",
            "iter 30: loss 10.8146, time 534.24ms, mfu 0.23%\n",
            "iter 31: loss 10.8135, time 550.26ms, mfu 0.23%\n",
            "iter 32: loss 10.8197, time 546.20ms, mfu 0.23%\n",
            "iter 33: loss 10.8066, time 538.28ms, mfu 0.23%\n",
            "iter 34: loss 10.8230, time 548.56ms, mfu 0.24%\n",
            "iter 35: loss 10.8116, time 542.53ms, mfu 0.24%\n",
            "iter 36: loss 10.8245, time 551.19ms, mfu 0.24%\n",
            "iter 37: loss 10.8127, time 550.13ms, mfu 0.24%\n",
            "iter 38: loss 10.8244, time 551.87ms, mfu 0.24%\n",
            "iter 39: loss 10.8079, time 534.34ms, mfu 0.24%\n",
            "iter 40: loss 10.7997, time 568.39ms, mfu 0.24%\n",
            "iter 41: loss 10.8205, time 543.59ms, mfu 0.24%\n",
            "iter 42: loss 10.8017, time 668.08ms, mfu 0.24%\n",
            "iter 43: loss 10.7936, time 752.70ms, mfu 0.23%\n",
            "iter 44: loss 10.8007, time 731.14ms, mfu 0.23%\n",
            "iter 45: loss 10.7977, time 763.24ms, mfu 0.22%\n",
            "iter 46: loss 10.8010, time 824.22ms, mfu 0.22%\n",
            "iter 47: loss 10.8004, time 829.62ms, mfu 0.21%\n",
            "iter 48: loss 10.7814, time 593.66ms, mfu 0.21%\n",
            "iter 49: loss 10.7669, time 549.22ms, mfu 0.22%\n",
            "iter 50: loss 10.7817, time 540.50ms, mfu 0.22%\n",
            "iter 51: loss 10.7828, time 546.83ms, mfu 0.22%\n",
            "iter 52: loss 10.7725, time 545.91ms, mfu 0.23%\n",
            "iter 53: loss 10.7743, time 556.48ms, mfu 0.23%\n",
            "iter 54: loss 10.7579, time 557.19ms, mfu 0.23%\n",
            "iter 55: loss 10.7412, time 549.53ms, mfu 0.23%\n",
            "iter 56: loss 10.7552, time 559.22ms, mfu 0.23%\n",
            "iter 57: loss 10.7522, time 548.74ms, mfu 0.23%\n",
            "iter 58: loss 10.7494, time 553.17ms, mfu 0.23%\n",
            "iter 59: loss 10.7435, time 553.15ms, mfu 0.24%\n",
            "iter 60: loss 10.7444, time 549.90ms, mfu 0.24%\n",
            "iter 61: loss 10.7217, time 544.85ms, mfu 0.24%\n",
            "iter 62: loss 10.7096, time 549.39ms, mfu 0.24%\n",
            "iter 63: loss 10.7285, time 546.40ms, mfu 0.24%\n",
            "iter 64: loss 10.7205, time 574.21ms, mfu 0.24%\n",
            "iter 65: loss 10.7010, time 567.92ms, mfu 0.24%\n",
            "iter 66: loss 10.7213, time 708.94ms, mfu 0.24%\n",
            "iter 67: loss 10.7064, time 738.41ms, mfu 0.23%\n",
            "iter 68: loss 10.7024, time 749.69ms, mfu 0.23%\n",
            "iter 69: loss 10.7098, time 763.18ms, mfu 0.22%\n",
            "iter 70: loss 10.6855, time 850.93ms, mfu 0.21%\n",
            "iter 71: loss 10.6911, time 841.96ms, mfu 0.21%\n",
            "iter 72: loss 10.6815, time 588.20ms, mfu 0.21%\n",
            "iter 73: loss 10.6716, time 552.35ms, mfu 0.21%\n",
            "iter 74: loss 10.6870, time 552.47ms, mfu 0.22%\n",
            "iter 75: loss 10.6653, time 548.82ms, mfu 0.22%\n",
            "iter 76: loss 10.6678, time 561.88ms, mfu 0.22%\n",
            "iter 77: loss 10.6642, time 550.87ms, mfu 0.23%\n",
            "iter 78: loss 10.6548, time 555.81ms, mfu 0.23%\n",
            "iter 79: loss 10.6617, time 557.08ms, mfu 0.23%\n",
            "iter 80: loss 10.6696, time 568.82ms, mfu 0.23%\n",
            "iter 81: loss 10.6564, time 565.71ms, mfu 0.23%\n",
            "iter 82: loss 10.6624, time 569.52ms, mfu 0.23%\n",
            "iter 83: loss 10.6336, time 554.73ms, mfu 0.23%\n",
            "iter 84: loss 10.6463, time 555.19ms, mfu 0.23%\n",
            "iter 85: loss 10.6490, time 560.16ms, mfu 0.24%\n",
            "iter 86: loss 10.6311, time 541.61ms, mfu 0.24%\n",
            "iter 87: loss 10.6557, time 555.87ms, mfu 0.24%\n",
            "iter 88: loss 10.6214, time 542.19ms, mfu 0.24%\n",
            "iter 89: loss 10.6236, time 553.73ms, mfu 0.24%\n",
            "iter 90: loss 10.6076, time 774.35ms, mfu 0.23%\n",
            "iter 91: loss 10.6227, time 755.58ms, mfu 0.23%\n",
            "iter 92: loss 10.6270, time 751.30ms, mfu 0.22%\n",
            "iter 93: loss 10.6179, time 787.80ms, mfu 0.22%\n",
            "iter 94: loss 10.5937, time 838.98ms, mfu 0.21%\n",
            "iter 95: loss 10.6347, time 786.95ms, mfu 0.21%\n",
            "iter 96: loss 10.6033, time 559.86ms, mfu 0.21%\n",
            "iter 97: loss 10.5946, time 546.24ms, mfu 0.22%\n",
            "iter 98: loss 10.5810, time 561.24ms, mfu 0.22%\n",
            "iter 99: loss 10.5795, time 564.20ms, mfu 0.22%\n",
            "step 100: train loss 10.5858, val loss 10.6142\n",
            "saving checkpoint to out\n",
            "iter 100: loss 10.6066, time 2038.62ms, mfu 0.21%\n",
            "iter 101: loss 10.5957, time 556.15ms, mfu 0.21%\n",
            "iter 102: loss 10.5953, time 565.36ms, mfu 0.21%\n",
            "iter 103: loss 10.5855, time 551.15ms, mfu 0.22%\n",
            "iter 104: loss 10.5829, time 563.81ms, mfu 0.22%\n",
            "iter 105: loss 10.5746, time 545.26ms, mfu 0.22%\n",
            "iter 106: loss 10.5640, time 560.28ms, mfu 0.22%\n",
            "iter 107: loss 10.5753, time 539.02ms, mfu 0.23%\n",
            "iter 108: loss 10.5587, time 558.27ms, mfu 0.23%\n",
            "iter 109: loss 10.5578, time 549.23ms, mfu 0.23%\n",
            "iter 110: loss 10.5524, time 557.34ms, mfu 0.23%\n",
            "iter 111: loss 10.5499, time 760.56ms, mfu 0.23%\n",
            "iter 112: loss 10.5523, time 734.23ms, mfu 0.22%\n",
            "iter 113: loss 10.5404, time 734.82ms, mfu 0.22%\n",
            "iter 114: loss 10.5340, time 770.94ms, mfu 0.21%\n",
            "iter 115: loss 10.5238, time 874.85ms, mfu 0.21%\n",
            "iter 116: loss 10.5259, time 813.61ms, mfu 0.20%\n",
            "iter 117: loss 10.5109, time 548.48ms, mfu 0.21%\n",
            "iter 118: loss 10.5182, time 553.84ms, mfu 0.21%\n",
            "iter 119: loss 10.5183, time 551.20ms, mfu 0.22%\n",
            "iter 120: loss 10.5058, time 556.33ms, mfu 0.22%\n",
            "iter 121: loss 10.5120, time 563.59ms, mfu 0.22%\n",
            "iter 122: loss 10.5099, time 552.31ms, mfu 0.22%\n",
            "iter 123: loss 10.4823, time 554.44ms, mfu 0.23%\n",
            "iter 124: loss 10.5049, time 572.99ms, mfu 0.23%\n",
            "iter 125: loss 10.4732, time 572.44ms, mfu 0.23%\n",
            "iter 126: loss 10.5112, time 563.61ms, mfu 0.23%\n",
            "iter 127: loss 10.4732, time 552.94ms, mfu 0.23%\n",
            "iter 128: loss 10.4531, time 565.61ms, mfu 0.23%\n",
            "iter 129: loss 10.4962, time 546.19ms, mfu 0.23%\n",
            "iter 130: loss 10.4636, time 555.03ms, mfu 0.23%\n",
            "iter 131: loss 10.4451, time 561.35ms, mfu 0.24%\n",
            "iter 132: loss 10.4441, time 553.33ms, mfu 0.24%\n",
            "iter 133: loss 10.4360, time 557.41ms, mfu 0.24%\n",
            "iter 134: loss 10.4443, time 644.84ms, mfu 0.23%\n",
            "iter 135: loss 10.4302, time 760.26ms, mfu 0.23%\n",
            "iter 136: loss 10.4397, time 748.48ms, mfu 0.22%\n",
            "iter 137: loss 10.4243, time 750.68ms, mfu 0.22%\n",
            "iter 138: loss 10.4212, time 793.45ms, mfu 0.22%\n",
            "iter 139: loss 10.4275, time 826.17ms, mfu 0.21%\n",
            "iter 140: loss 10.4194, time 694.05ms, mfu 0.21%\n",
            "iter 141: loss 10.4261, time 543.21ms, mfu 0.21%\n",
            "iter 142: loss 10.4087, time 552.82ms, mfu 0.22%\n",
            "iter 143: loss 10.4299, time 549.56ms, mfu 0.22%\n",
            "iter 144: loss 10.4010, time 554.86ms, mfu 0.22%\n",
            "iter 145: loss 10.3820, time 545.45ms, mfu 0.22%\n",
            "iter 146: loss 10.3942, time 550.40ms, mfu 0.23%\n",
            "iter 147: loss 10.3959, time 539.52ms, mfu 0.23%\n",
            "iter 148: loss 10.3649, time 560.48ms, mfu 0.23%\n",
            "iter 149: loss 10.3830, time 553.66ms, mfu 0.23%\n",
            "iter 150: loss 10.3711, time 565.50ms, mfu 0.23%\n",
            "iter 151: loss 10.3714, time 578.54ms, mfu 0.23%\n",
            "iter 152: loss 10.3445, time 552.05ms, mfu 0.23%\n",
            "iter 153: loss 10.3342, time 560.18ms, mfu 0.24%\n",
            "iter 154: loss 10.3337, time 555.57ms, mfu 0.24%\n",
            "iter 155: loss 10.3404, time 554.39ms, mfu 0.24%\n",
            "iter 156: loss 10.3471, time 552.88ms, mfu 0.24%\n",
            "iter 157: loss 10.3282, time 564.79ms, mfu 0.24%\n",
            "iter 158: loss 10.3280, time 680.04ms, mfu 0.23%\n",
            "iter 159: loss 10.3062, time 749.84ms, mfu 0.23%\n",
            "iter 160: loss 10.3217, time 745.58ms, mfu 0.22%\n",
            "iter 161: loss 10.3038, time 772.43ms, mfu 0.22%\n",
            "iter 162: loss 10.3041, time 791.61ms, mfu 0.21%\n",
            "iter 163: loss 10.2835, time 818.83ms, mfu 0.21%\n",
            "iter 164: loss 10.3024, time 636.46ms, mfu 0.21%\n",
            "iter 165: loss 10.2975, time 549.68ms, mfu 0.21%\n",
            "iter 166: loss 10.2827, time 555.68ms, mfu 0.22%\n",
            "iter 167: loss 10.2665, time 549.01ms, mfu 0.22%\n",
            "iter 168: loss 10.2783, time 539.51ms, mfu 0.22%\n",
            "iter 169: loss 10.2543, time 585.31ms, mfu 0.22%\n",
            "iter 170: loss 10.2694, time 546.73ms, mfu 0.23%\n",
            "iter 171: loss 10.2602, time 563.51ms, mfu 0.23%\n",
            "iter 172: loss 10.2491, time 545.03ms, mfu 0.23%\n",
            "iter 173: loss 10.2244, time 561.67ms, mfu 0.23%\n",
            "iter 174: loss 10.2508, time 554.08ms, mfu 0.23%\n",
            "iter 175: loss 10.2210, time 552.23ms, mfu 0.23%\n",
            "iter 176: loss 10.2221, time 533.49ms, mfu 0.24%\n",
            "iter 177: loss 10.1876, time 558.44ms, mfu 0.24%\n",
            "iter 178: loss 10.2117, time 551.94ms, mfu 0.24%\n",
            "iter 179: loss 10.1757, time 552.54ms, mfu 0.24%\n",
            "iter 180: loss 10.2239, time 549.12ms, mfu 0.24%\n",
            "iter 181: loss 10.1973, time 557.22ms, mfu 0.24%\n",
            "iter 182: loss 10.1861, time 688.62ms, mfu 0.24%\n",
            "iter 183: loss 10.1705, time 716.94ms, mfu 0.23%\n",
            "iter 184: loss 10.1535, time 761.06ms, mfu 0.23%\n",
            "iter 185: loss 10.1628, time 776.56ms, mfu 0.22%\n",
            "iter 186: loss 10.1474, time 815.43ms, mfu 0.22%\n",
            "iter 187: loss 10.1618, time 825.11ms, mfu 0.21%\n",
            "iter 188: loss 10.1215, time 598.49ms, mfu 0.21%\n",
            "iter 189: loss 10.1451, time 538.84ms, mfu 0.22%\n",
            "iter 190: loss 10.1377, time 548.37ms, mfu 0.22%\n",
            "iter 191: loss 10.0963, time 542.00ms, mfu 0.22%\n",
            "iter 192: loss 10.1194, time 531.58ms, mfu 0.23%\n",
            "iter 193: loss 10.1185, time 555.77ms, mfu 0.23%\n",
            "iter 194: loss 10.1200, time 542.33ms, mfu 0.23%\n",
            "iter 195: loss 10.0799, time 548.14ms, mfu 0.23%\n",
            "iter 196: loss 10.0710, time 538.74ms, mfu 0.23%\n",
            "iter 197: loss 10.0679, time 542.05ms, mfu 0.24%\n",
            "iter 198: loss 10.0592, time 544.26ms, mfu 0.24%\n",
            "iter 199: loss 10.0841, time 540.37ms, mfu 0.24%\n",
            "step 200: train loss 10.0686, val loss 10.1812\n",
            "saving checkpoint to out\n",
            "iter 200: loss 10.0757, time 2042.13ms, mfu 0.22%\n",
            "iter 201: loss 10.0662, time 552.84ms, mfu 0.22%\n",
            "iter 202: loss 10.0670, time 549.64ms, mfu 0.23%\n",
            "iter 203: loss 10.0489, time 622.10ms, mfu 0.23%\n",
            "iter 204: loss 10.0522, time 744.78ms, mfu 0.22%\n",
            "iter 205: loss 10.0485, time 743.45ms, mfu 0.22%\n",
            "iter 206: loss 10.0175, time 743.41ms, mfu 0.21%\n",
            "iter 207: loss 10.0259, time 777.85ms, mfu 0.21%\n",
            "iter 208: loss 10.0502, time 824.94ms, mfu 0.21%\n",
            "iter 209: loss 9.9926, time 694.06ms, mfu 0.20%\n",
            "iter 210: loss 9.9985, time 546.68ms, mfu 0.21%\n",
            "iter 211: loss 9.9609, time 544.79ms, mfu 0.21%\n",
            "iter 212: loss 9.9812, time 534.49ms, mfu 0.22%\n",
            "iter 213: loss 9.9826, time 544.28ms, mfu 0.22%\n",
            "iter 214: loss 9.9581, time 536.80ms, mfu 0.22%\n",
            "iter 215: loss 9.9548, time 548.50ms, mfu 0.23%\n",
            "iter 216: loss 9.9581, time 531.21ms, mfu 0.23%\n",
            "iter 217: loss 9.9619, time 539.66ms, mfu 0.23%\n",
            "iter 218: loss 9.9149, time 547.31ms, mfu 0.23%\n",
            "iter 219: loss 9.9516, time 559.55ms, mfu 0.23%\n",
            "iter 220: loss 9.9536, time 546.33ms, mfu 0.24%\n",
            "iter 221: loss 9.9208, time 552.31ms, mfu 0.24%\n",
            "iter 222: loss 9.9063, time 540.46ms, mfu 0.24%\n",
            "iter 223: loss 9.9195, time 546.68ms, mfu 0.24%\n",
            "iter 224: loss 9.8780, time 542.42ms, mfu 0.24%\n",
            "iter 225: loss 9.8996, time 525.07ms, mfu 0.24%\n",
            "iter 226: loss 9.8594, time 554.80ms, mfu 0.24%\n",
            "iter 227: loss 9.8763, time 568.04ms, mfu 0.24%\n",
            "iter 228: loss 9.9002, time 746.37ms, mfu 0.24%\n",
            "iter 229: loss 9.8483, time 740.48ms, mfu 0.23%\n",
            "iter 230: loss 9.8453, time 720.61ms, mfu 0.23%\n",
            "iter 231: loss 9.8373, time 767.31ms, mfu 0.22%\n",
            "iter 232: loss 9.8668, time 823.12ms, mfu 0.22%\n",
            "iter 233: loss 9.8373, time 782.29ms, mfu 0.21%\n",
            "iter 234: loss 9.8058, time 527.04ms, mfu 0.22%\n",
            "iter 235: loss 9.8160, time 556.45ms, mfu 0.22%\n",
            "iter 236: loss 9.8155, time 538.65ms, mfu 0.22%\n",
            "iter 237: loss 9.8173, time 549.21ms, mfu 0.23%\n",
            "iter 238: loss 9.7983, time 561.13ms, mfu 0.23%\n",
            "iter 239: loss 9.7717, time 548.86ms, mfu 0.23%\n",
            "iter 240: loss 9.8074, time 532.07ms, mfu 0.23%\n",
            "iter 241: loss 9.7782, time 542.57ms, mfu 0.23%\n",
            "iter 242: loss 9.7612, time 535.45ms, mfu 0.24%\n",
            "iter 243: loss 9.7657, time 542.87ms, mfu 0.24%\n",
            "iter 244: loss 9.7202, time 550.08ms, mfu 0.24%\n",
            "iter 245: loss 9.7380, time 546.20ms, mfu 0.24%\n",
            "iter 246: loss 9.7650, time 556.34ms, mfu 0.24%\n",
            "iter 247: loss 9.7227, time 536.68ms, mfu 0.24%\n",
            "iter 248: loss 9.7207, time 553.14ms, mfu 0.24%\n",
            "iter 249: loss 9.6758, time 544.81ms, mfu 0.24%\n",
            "iter 250: loss 9.7095, time 551.11ms, mfu 0.24%\n",
            "iter 251: loss 9.7368, time 543.94ms, mfu 0.24%\n",
            "iter 252: loss 9.6806, time 750.47ms, mfu 0.24%\n",
            "iter 253: loss 9.6697, time 755.69ms, mfu 0.23%\n",
            "iter 254: loss 9.6483, time 726.36ms, mfu 0.23%\n",
            "iter 255: loss 9.6842, time 779.42ms, mfu 0.22%\n",
            "iter 256: loss 9.6463, time 841.43ms, mfu 0.22%\n",
            "iter 257: loss 9.6547, time 812.92ms, mfu 0.21%\n",
            "iter 258: loss 9.6311, time 536.92ms, mfu 0.22%\n",
            "iter 259: loss 9.6451, time 537.15ms, mfu 0.22%\n",
            "iter 260: loss 9.6390, time 540.98ms, mfu 0.22%\n",
            "iter 261: loss 9.6154, time 537.80ms, mfu 0.23%\n",
            "iter 262: loss 9.6281, time 553.15ms, mfu 0.23%\n",
            "iter 263: loss 9.6045, time 559.85ms, mfu 0.23%\n",
            "iter 264: loss 9.5701, time 564.68ms, mfu 0.23%\n",
            "iter 265: loss 9.5952, time 546.32ms, mfu 0.23%\n",
            "iter 266: loss 9.5794, time 558.06ms, mfu 0.23%\n",
            "iter 267: loss 9.5589, time 537.71ms, mfu 0.24%\n",
            "iter 268: loss 9.5366, time 553.19ms, mfu 0.24%\n",
            "iter 269: loss 9.5476, time 557.58ms, mfu 0.24%\n",
            "iter 270: loss 9.5484, time 552.63ms, mfu 0.24%\n",
            "iter 271: loss 9.5544, time 536.60ms, mfu 0.24%\n",
            "iter 272: loss 9.5423, time 550.91ms, mfu 0.24%\n",
            "iter 273: loss 9.5396, time 551.31ms, mfu 0.24%\n",
            "iter 274: loss 9.5049, time 561.06ms, mfu 0.24%\n",
            "iter 275: loss 9.4912, time 536.78ms, mfu 0.24%\n",
            "iter 276: loss 9.5112, time 730.99ms, mfu 0.24%\n",
            "iter 277: loss 9.4901, time 731.49ms, mfu 0.23%\n",
            "iter 278: loss 9.4424, time 730.65ms, mfu 0.23%\n",
            "iter 279: loss 9.4714, time 766.69ms, mfu 0.22%\n",
            "iter 280: loss 9.4399, time 838.28ms, mfu 0.22%\n",
            "iter 281: loss 9.4623, time 824.46ms, mfu 0.21%\n",
            "iter 282: loss 9.4015, time 544.29ms, mfu 0.21%\n",
            "iter 283: loss 9.4266, time 549.89ms, mfu 0.22%\n",
            "iter 284: loss 9.3813, time 551.63ms, mfu 0.22%\n",
            "iter 285: loss 9.4138, time 550.61ms, mfu 0.22%\n",
            "iter 286: loss 9.4035, time 544.43ms, mfu 0.23%\n",
            "iter 287: loss 9.3730, time 559.56ms, mfu 0.23%\n",
            "iter 288: loss 9.3540, time 555.08ms, mfu 0.23%\n",
            "iter 289: loss 9.3714, time 571.34ms, mfu 0.23%\n",
            "iter 290: loss 9.3709, time 554.00ms, mfu 0.23%\n",
            "iter 291: loss 9.3711, time 559.40ms, mfu 0.23%\n",
            "iter 292: loss 9.3237, time 550.16ms, mfu 0.23%\n",
            "iter 293: loss 9.3309, time 544.41ms, mfu 0.24%\n",
            "iter 294: loss 9.3296, time 540.55ms, mfu 0.24%\n",
            "iter 295: loss 9.3092, time 536.12ms, mfu 0.24%\n",
            "iter 296: loss 9.3425, time 549.47ms, mfu 0.24%\n",
            "iter 297: loss 9.2942, time 534.90ms, mfu 0.24%\n",
            "iter 298: loss 9.2940, time 558.21ms, mfu 0.24%\n",
            "iter 299: loss 9.2772, time 536.04ms, mfu 0.24%\n",
            "step 300: train loss 9.2738, val loss 9.5170\n",
            "saving checkpoint to out\n",
            "iter 300: loss 9.2717, time 2652.50ms, mfu 0.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print 5 samples, with 10 tokens, starting with \"to be\"\n",
        "!cd ./nanoGPT && python sample.py --dtype=float16 --num_samples=5 --max_new_tokens=10 --start=\"Tesla is\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO95LseO1oRg",
        "outputId": "f9848388-d7e2-4814-b508-c2f4e8428b0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 10\n",
            "Overriding: start = Tesla is\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
            "number of parameters: 3.42M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Tesla is September sell Institute Tar four 2021 many have 2020 be\n",
            "---------------\n",
            "Tesla is for Tesla recommended all each for andly solar build\n",
            "---------------\n",
            "Tesla is reached E proposedilot Full less E North data revealed\n",
            "---------------\n",
            "Tesla is an cities sports Motors February Statehard recommended through cities\n",
            "---------------\n",
            "Tesla is components D advertising for revealed by three 40 China Tar\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}